{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "News_clf.ipynb",
      "provenance": [],
      "mount_file_id": "1Dooj3BGvTzqMHwwgsGx7pSUdFPiY25ut",
      "authorship_tag": "ABX9TyPV9OWyogpj4C9eqAmDIXUY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karthikRavichandran/News_classfication/blob/master/News_clf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGYtHyBCBR23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D, MaxPooling1D\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Conv1D, Dropout, Bidirectional\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow import math\n",
        "import tqdm\n",
        "import re\n",
        "import string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQBwgyQPCUVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# d_f = pd.read_csv(\"/content/news.csv\", sep=\"\\t\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_UA0jMAIxn_",
        "colab_type": "code",
        "outputId": "6888af66-0cf2-406a-8bf5-44c31ebd2c4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "# reading data from News sentiment analysis data set downloaded from kaggle \n",
        "d_f2 = pd.read_csv(\"/content/drive/My Drive/MODELS/all-data.csv\")\n",
        "d_f2.columns = [\"classes\", \"News\"]\n",
        "d_f2[\"classes_num\"]= d_f2.classes.map({'neutral': 1, 'negative': 0, 'positive' : 2})\n",
        "d_f2[\"Positive\"] = np.where(d_f2[\"classes\"].str.contains(\"positive\"), 1,0)\n",
        "d_f2[\"negative\"] = np.where(d_f2[\"classes\"].str.contains(\"negative\"), 1,0)\n",
        "d_f2[\"neutral\"] = np.where(d_f2[\"classes\"].str.contains(\"neutral\"), 1,0)\n",
        "d_f2.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>classes</th>\n",
              "      <th>News</th>\n",
              "      <th>classes_num</th>\n",
              "      <th>Positive</th>\n",
              "      <th>negative</th>\n",
              "      <th>neutral</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neutral</td>\n",
              "      <td>Technopolis plans to develop in stages an area...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>negative</td>\n",
              "      <td>The international electronic industry company ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>positive</td>\n",
              "      <td>With the new production plant the company woul...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>positive</td>\n",
              "      <td>According to the company 's updated strategy f...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>positive</td>\n",
              "      <td>FINANCING OF ASPOCOMP 'S GROWTH Aspocomp is ag...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    classes  ... neutral\n",
              "0   neutral  ...       1\n",
              "1  negative  ...       0\n",
              "2  positive  ...       0\n",
              "3  positive  ...       0\n",
              "4  positive  ...       0\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeJQqDlbUniD",
        "colab_type": "code",
        "outputId": "a6238d34-29cc-4e26-ed48-6150318dbf7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "np.unique(d_f2[\"classes\"], return_counts = True) #class bias analysis"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['negative', 'neutral', 'positive'], dtype=object),\n",
              " array([ 604, 2878, 1363]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjDHyQ5TLSQK",
        "colab_type": "code",
        "outputId": "a02ac948-2350-4f70-8c09-6f6a16c0089f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "#text preprocessing \n",
        "def clean_text_round1(text):\n",
        "    '''Make text lowercase, remove text in square brackets, remove punctuation.\n",
        "      Input to the clean_text_round1 a sentance with spl char\n",
        "      Output is a clean text by removing spl char '''\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('[‘’“”…]', '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', 'numberstring', text)\n",
        "    return text\n",
        "for i in tqdm.tqdm(range(d_f2['News'].shape[0])):\n",
        "#     print(i)\n",
        "    d_f2['News'].iloc[i] = clean_text_round1(d_f2['News'].iloc[i])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/4845 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:671: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_with_indexer(indexer, value)\n",
            "100%|██████████| 4845/4845 [00:01<00:00, 4213.14it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkCiC-ElGdZ_",
        "colab_type": "code",
        "outputId": "a0597992-6e78-4dbe-ebc2-2501142395c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "learn_text = \" [%] (I am?) in .... ////r><><> march2020?\"\n",
        "learn_text = clean_text_round1(learn_text)\n",
        "\n",
        "print(learn_text) "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  i am in  r numberstring\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJy6wMvIvAul",
        "colab_type": "code",
        "outputId": "87ea5542-b515-40e9-ff73-e025f39c1eae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "#Reading data from IMDB comment sentiment analysis data set\n",
        "df = pd.read_csv(\"/content/drive/My Drive/MODELS/train.csv\",sep=\"\\t\" )\n",
        "df[\"Positive\"] = (df[\"Sentiment\"] > 2).astype(int)\n",
        "df[\"Negative\"] = (df[\"Sentiment\"] < 2).astype(int)\n",
        "df[\"neutral\"] = (df[\"Sentiment\"] == 2).astype(int)\n",
        "train2 = df[\"Phrase\"]\n",
        "train2_Y = df.drop([\"PhraseId\", \"SentenceId\", \"Phrase\", \"Sentiment\"], axis =1).values\n",
        "for i in tqdm.tqdm(range(train2.shape[0])):\n",
        "#     print(i)\n",
        "    train2.iloc[i] = clean_text_round1(train2.iloc[i])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/156060 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:671: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_with_indexer(indexer, value)\n",
            "100%|██████████| 156060/156060 [02:10<00:00, 1192.58it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LplMqB5TP6g4",
        "colab_type": "code",
        "outputId": "3ad464b4-a7a1-4a5f-b46a-7db8efc71826",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "d_f2.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>classes</th>\n",
              "      <th>News</th>\n",
              "      <th>classes_num</th>\n",
              "      <th>Positive</th>\n",
              "      <th>negative</th>\n",
              "      <th>neutral</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neutral</td>\n",
              "      <td>technopolis plans to develop in stages an area...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>negative</td>\n",
              "      <td>the international electronic industry company ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>positive</td>\n",
              "      <td>with the new production plant the company woul...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>positive</td>\n",
              "      <td>according to the company s updated strategy fo...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>positive</td>\n",
              "      <td>financing of aspocomp s growth aspocomp is agg...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    classes  ... neutral\n",
              "0   neutral  ...       1\n",
              "1  negative  ...       0\n",
              "2  positive  ...       0\n",
              "3  positive  ...       0\n",
              "4  positive  ...       0\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F61rrFddRIHf",
        "colab_type": "code",
        "outputId": "30d3c247-139e-4208-b125-645c8a035fbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Adding both the data sets together in a dataframe\n",
        "three_class = d_f2.drop([\"News\", \"classes_num\", \"classes\",], axis = 1).values\n",
        "print(three_class.shape)\n",
        "three_class = np.append(three_class, train2_Y, axis =0)\n",
        "print(np.shape(three_class))\n",
        "News_max = np.append(d_f2['News'], train2, axis = 0)\n",
        "print(np.shape(News_max))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4845, 3)\n",
            "(160905, 3)\n",
            "(160905,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Btg5R9qERzme",
        "colab_type": "code",
        "outputId": "f5bd006c-aacb-4196-f731-27234835bec6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "#Splitting data set for train and test\n",
        "df_train_max, df_test, Y_multi_train_max, Y_multi_test = train_test_split(News_max, three_class, test_size=0.33, random_state = 42)\n",
        "print(\"Train shape : \", df_train_max.shape, \"test shape : \", df_test.shape)\n",
        "print(\"Train shape : \", Y_multi_train_max.shape, \"test shape : \", Y_multi_test.shape)\n",
        "print(\"Class balance : \",Y_multi_train_max[:,0].sum(),Y_multi_train_max[:,1].sum(), Y_multi_train_max[:,2].sum() )\n",
        "# print(Y_multi_train_max[:,0])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape :  (107806,) test shape :  (53099,)\n",
            "Train shape :  (107806, 3) test shape :  (53099, 3)\n",
            "Class balance :  29161 23373 55272\n",
            "[0 0 1 ... 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gZxpLcTSUDY",
        "colab_type": "code",
        "outputId": "f7e1d7a0-e191-4d22-8787-ebf19d6ab9e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#further splitting train_max into validation and train. \n",
        "df_train, df_val, Y_multi_train, Y_multi_val = train_test_split(df_train_max, Y_multi_train_max, test_size=0.33, random_state = 42)\n",
        "print(\"Train shape : \", df_train.shape, \"val shape : \", df_val.shape)\n",
        "print(\"Train shape : \", Y_multi_train.shape, \"val shape : \", Y_multi_val.shape)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape :  (72230,) val shape :  (35576,)\n",
            "Train shape :  (72230, 3) val shape :  (35576, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z304f1pXWmc-",
        "colab_type": "code",
        "outputId": "1486032f-a8d6-41fd-8f1c-bbdfeccc3fb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# text preprocessing for converting text in to vector\n",
        "import json\n",
        "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
        "MAX_VOCAB_SIZE = 20000\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts(df_train)# fit on training data\n",
        "sequences_train = tokenizer.texts_to_sequences(df_train)# transform on fitted Tokenizer\n",
        "sequences_test = tokenizer.texts_to_sequences(df_test)# transform on fitted Tokenizer\n",
        "sequences_val = tokenizer.texts_to_sequences(df_val)# transform on fitted Tokenizer\n",
        "max_ = 0\n",
        "max_ = max([len(seq) for seq in sequences_train])\n",
        "word2idx = tokenizer.word_index\n",
        "V = len(word2idx)\n",
        "print(\"length of max sequence : \", max_)\n",
        "print('Found %s unique tokens.' % V)\n",
        "file_path = \"/content/drive/My Drive/MODELS/\"\n",
        "print('saving tokenizer to json file =======> ', file_path)\n",
        "tokenizer_json = tokenizer.to_json()\n",
        "with open(file_path +'/tokenizer.json', 'w', encoding='utf-8') as f:\n",
        "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
        "\n",
        "print(\"Loading file tokenizer from json =======> \", file_path +'/tokenizer.json')\n",
        "with open(file_path + '/tokenizer.json') as f:\n",
        "    data = json.load(f)\n",
        "    tokenizer = tokenizer_from_json(data)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of max sequence :  52\n",
            "Found 19519 unique tokens.\n",
            "saving tokenizer to json file =======>  /content/drive/My Drive/MODELS/\n",
            "Loading file tokenizer from json =======>  /content/drive/My Drive/MODELS//tokenizer.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3s0XZpCKZBAI",
        "colab_type": "code",
        "outputId": "d2153205-2b0d-45db-a282-b73c5484df63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# pad sequences zeros\n",
        "data_train = pad_sequences(sequences_train, maxlen = max_)\n",
        "print('Shape of data train tensor:', data_train.shape)\n",
        "data_test = pad_sequences(sequences_test, maxlen = max_)\n",
        "print('Shape of data train tensor:', data_test.shape)\n",
        "data_val = pad_sequences(sequences_val, maxlen = max_)\n",
        "print('Shape of data train tensor:', data_val.shape)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data train tensor: (72230, 52)\n",
            "Shape of data train tensor: (53099, 52)\n",
            "Shape of data train tensor: (35576, 52)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnvTNhcgajTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIEmuuzCa5bo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.keras.backend as K\n",
        "from sklearn.utils import class_weight\n",
        "# class_weight = class_weight.compute_class_weight('balanced'\n",
        "#                                                ,np.unique(Ytrain)\n",
        "#                                                ,Ytrain)\n",
        "\n",
        "def get_f1(y_true, y_pred): \n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val \n",
        "# Here I used f1 score in metrics\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkdUCO6z6k8a",
        "colab_type": "code",
        "outputId": "a034734b-e760-4813-b722-84428b5f9cf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "#Download glove pretrained weights\n",
        "! wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-28 16:28:26--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-05-28 16:28:26--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-05-28 16:28:27--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  1.97MB/s    in 6m 29s  \n",
            "\n",
            "2020-05-28 16:34:56 (2.11 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "we8GkiJM8yfG",
        "colab_type": "code",
        "outputId": "1edb80c4-0485-4e24-c83e-2dc80715f83c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "#unzip the file from downloaded glove zip file\n",
        "from zipfile import ZipFile   \n",
        "# specifying the zip file name \n",
        "file_name = \"/content/glove.6B.zip\"\n",
        "# opening the zip file in READ mode \n",
        "with ZipFile(file_name, 'r') as zip: \n",
        "    # printing all the contents of the zip file \n",
        "    zip.printdir() \n",
        "    # extracting all the files \n",
        "    print('Extracting all the files now...') \n",
        "    zip.extractall() \n",
        "    print('Done!') "
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File Name                                             Modified             Size\n",
            "glove.6B.50d.txt                               2014-08-04 13:15:00    171350079\n",
            "glove.6B.100d.txt                              2014-08-04 13:14:34    347116733\n",
            "glove.6B.200d.txt                              2014-08-04 13:14:44    693432828\n",
            "glove.6B.300d.txt                              2014-08-27 12:19:16   1037962819\n",
            "Extracting all the files now...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BenhvIAw9Ua3",
        "colab_type": "code",
        "outputId": "f6f9468c-51bb-43ef-cf82-a7c92ef0e239",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# load glove pretained weights in to dict with word and its vector rep.\n",
        "import os\n",
        "embeddings_index = {}\n",
        "GLOVE_DIR = \"/content/\"\n",
        "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "word_index = word2idx\n",
        "print(len(word2idx))\n",
        "EMBEDDING_DIM = 100\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n",
            "19519\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfh6pDPU-mN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use the glove weight to the embedding layer (transfer learning)\n",
        "from tensorflow.keras.layers import Embedding\n",
        "MAX_SEQUENCE_LENGTH = T\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=True) #trainable can be freezed if we want"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNCqIZQMmPqd",
        "colab_type": "code",
        "outputId": "db380ecb-4b11-4f4d-cbc7-67aede9c4764",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# # Checking GPU !\n",
        "# !cat /proc/meminfo\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKd_B4Vr_Rho",
        "colab_type": "code",
        "outputId": "0a6799df-8a09-4b86-acdd-4a8e4410471e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        }
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "T = data_train.shape[1] # or max_\n",
        "# embedding dimensionality\n",
        "D = 100 \n",
        "# Hidden state dimensionality\n",
        "H = 200\n",
        "#==================================== tried different arch =============================\n",
        "\n",
        "# sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "# embedded_sequences = embedding_layer(sequence_input)\n",
        "# x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
        "# x = MaxPooling1D(5)(x)\n",
        "# x = Conv1D(128, 5, activation='relu')(x)\n",
        "# x = MaxPooling1D(5)(x)\n",
        "# x = Conv1D(128, 5, activation='relu')(x)\n",
        "# x = MaxPooling1D(35)(x)  # global max pooling\n",
        "# x = Flatten()(x)\n",
        "# x = Dense(128, activation='relu')(x)\n",
        "# preds = Dense(len(labels_index), activation='softmax')(x)\n",
        "#==========================================================================================\n",
        "\n",
        "\n",
        "i = Input(shape=(T,))\n",
        "x = embedding_layer(i) # we can use glove pretrained weights for this embedding(transfer learning)\n",
        "                            # freez the weights\n",
        "x = LSTM(H, return_sequences=True)(x)\n",
        "x = Bidirectional(LSTM(100, return_sequences=True, dropout = 0.50), merge_mode='concat')(x)\n",
        "# x = LSTM(50, return_sequences=True)(x)\n",
        "# x = Conv1D(32, 3, activation='relu')(x)\n",
        "# x = MaxPooling1D(3)(x)\n",
        "# x = Conv1D(64, 3, activation='relu')(x)\n",
        "# x = MaxPooling1D(3)(x)\n",
        "# x = Conv1D(128, 3, activation='relu')(x)\n",
        "x = GlobalMaxPooling1D()(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(16, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(3, activation='softmax')(x)\n",
        "# x = Dropout(0.2)(x)\n",
        "\n",
        "model = Model(i, x)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=[get_f1]) # [get_f1] \n",
        "\n",
        "# filepath = \"/content/\"\n",
        "filepath=\"/content/weights-improvement-f1-{epoch:02d}-{val_get_f1:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_get_f1', \n",
        "                             verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "r = model.fit(\n",
        "  # np.append(data_train, data_test, axis = 0),\n",
        "  # np.append(Y_multi_train, Y_multi_test, axis =0),\n",
        "  data_train, Y_multi_train,\n",
        "  epochs=10,\n",
        "  validation_data=(data_val, np.array(Y_multi_val),),\n",
        "  batch_size=64, verbose=1, shuffle=True, callbacks=callbacks_list\n",
        ")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1129/1129 [==============================] - ETA: 0s - loss: 0.6962 - get_f1: 0.6617\n",
            "Epoch 00001: val_get_f1 improved from -inf to 0.72521, saving model to /content/weights-improvement-f1-01-0.73.hdf5\n",
            "1129/1129 [==============================] - 38s 33ms/step - loss: 0.6962 - get_f1: 0.6617 - val_loss: 0.6321 - val_get_f1: 0.7252\n",
            "Epoch 2/10\n",
            "1129/1129 [==============================] - ETA: 0s - loss: 0.5764 - get_f1: 0.7556\n",
            "Epoch 00002: val_get_f1 improved from 0.72521 to 0.72870, saving model to /content/weights-improvement-f1-02-0.73.hdf5\n",
            "1129/1129 [==============================] - 36s 32ms/step - loss: 0.5764 - get_f1: 0.7556 - val_loss: 0.6288 - val_get_f1: 0.7287\n",
            "Epoch 3/10\n",
            "1129/1129 [==============================] - ETA: 0s - loss: 0.5171 - get_f1: 0.7817\n",
            "Epoch 00003: val_get_f1 did not improve from 0.72870\n",
            "1129/1129 [==============================] - 36s 32ms/step - loss: 0.5171 - get_f1: 0.7817 - val_loss: 0.6494 - val_get_f1: 0.7274\n",
            "Epoch 4/10\n",
            "1129/1129 [==============================] - ETA: 0s - loss: 0.4733 - get_f1: 0.8016\n",
            "Epoch 00004: val_get_f1 improved from 0.72870 to 0.72948, saving model to /content/weights-improvement-f1-04-0.73.hdf5\n",
            "1129/1129 [==============================] - 35s 31ms/step - loss: 0.4733 - get_f1: 0.8016 - val_loss: 0.6617 - val_get_f1: 0.7295\n",
            "Epoch 5/10\n",
            "1129/1129 [==============================] - ETA: 0s - loss: 0.4373 - get_f1: 0.8155\n",
            "Epoch 00005: val_get_f1 improved from 0.72948 to 0.73475, saving model to /content/weights-improvement-f1-05-0.73.hdf5\n",
            "1129/1129 [==============================] - 35s 31ms/step - loss: 0.4373 - get_f1: 0.8155 - val_loss: 0.7304 - val_get_f1: 0.7347\n",
            "Epoch 6/10\n",
            "1129/1129 [==============================] - ETA: 0s - loss: 0.4043 - get_f1: 0.8280\n",
            "Epoch 00006: val_get_f1 did not improve from 0.73475\n",
            "1129/1129 [==============================] - 37s 32ms/step - loss: 0.4043 - get_f1: 0.8280 - val_loss: 0.7702 - val_get_f1: 0.7328\n",
            "Epoch 7/10\n",
            "1128/1129 [============================>.] - ETA: 0s - loss: 0.3758 - get_f1: 0.8401\n",
            "Epoch 00007: val_get_f1 did not improve from 0.73475\n",
            "1129/1129 [==============================] - 36s 32ms/step - loss: 0.3758 - get_f1: 0.8401 - val_loss: 0.8559 - val_get_f1: 0.7331\n",
            "Epoch 8/10\n",
            "1129/1129 [==============================] - ETA: 0s - loss: 0.3528 - get_f1: 0.8470\n",
            "Epoch 00008: val_get_f1 did not improve from 0.73475\n",
            "1129/1129 [==============================] - 35s 31ms/step - loss: 0.3528 - get_f1: 0.8470 - val_loss: 0.9068 - val_get_f1: 0.7288\n",
            "Epoch 9/10\n",
            "1129/1129 [==============================] - ETA: 0s - loss: 0.3278 - get_f1: 0.8569\n",
            "Epoch 00009: val_get_f1 did not improve from 0.73475\n",
            "1129/1129 [==============================] - 35s 31ms/step - loss: 0.3278 - get_f1: 0.8569 - val_loss: 1.2065 - val_get_f1: 0.7256\n",
            "Epoch 10/10\n",
            "1129/1129 [==============================] - ETA: 0s - loss: 0.3122 - get_f1: 0.8631\n",
            "Epoch 00010: val_get_f1 did not improve from 0.73475\n",
            "1129/1129 [==============================] - 36s 32ms/step - loss: 0.3122 - get_f1: 0.8631 - val_loss: 1.2637 - val_get_f1: 0.7213\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O1XgZx2tlPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_test = model.predict(data_test)>0.5 #predicting test data in trained model\n",
        "predicted_test = predicted_test*1\n",
        "predicted_val = model.predict(data_val)>0.5 #predicting test data in trained model\n",
        "predicted_val = predicted_val*1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U81BIBapNV-5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "b379949e-fac8-4a58-9991-334bba52e0cb"
      },
      "source": [
        "#Inspecting confusion matrix for val data set as well as test data set\n",
        "val_con = math.confusion_matrix(Y_multi_test.argmax(axis = 1), predicted_test.argmax(axis = 1)) \n",
        "test_con = math.confusion_matrix(Y_multi_val.argmax(axis = 1), predicted_val.argmax(axis = 1)) \n",
        "print(val_con)\n",
        "print(test_con)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[10305   368  3662]\n",
            " [  599  8605  2372]\n",
            " [ 3651  3776 19761]], shape=(3, 3), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[ 6832   237  2493]\n",
            " [  379  5643  1738]\n",
            " [ 2548  2580 13126]], shape=(3, 3), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntty_CfEP62u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prediction_(text, model, T):\n",
        "    sequences_test_2 = tokenizer.texts_to_sequences([text])\n",
        "    sequences_test_2 = pad_sequences(sequences_test_2, maxlen = max_)\n",
        "    print(\"prediction : \", model.predict(sequences_test_2))\n",
        "    return np.argmax(model.predict(sequences_test_2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQbQPnm66CmG",
        "colab_type": "code",
        "outputId": "7b5dc0ff-cdd9-4c29-9194-ac6a988a8d08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "test = \"all the days the stock went low and it did not improve even at the end of the day\"\n",
        "test = clean_text_round1(test)\n",
        "# print(test)\n",
        "print(test)\n",
        "test_sen = np.argmax(prediction_(test, model, T))\n",
        "print(test_sen)\n",
        "if test_sen == 0:\n",
        "  print(\"Postive sentance \")\n",
        "elif test_sen == 1:\n",
        "  print(\"negative sentance\")\n",
        "else:\n",
        "  print(\"neutral\")\n",
        "# predict of the above sentance in negative and it's true."
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all the days the stock went low and it did not improve even at the end of the day\n",
            "prediction :  [[6.2096942e-06 8.6855495e-01 1.3143887e-01]]\n",
            "0\n",
            "Postive sentance \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}